{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation with Optimal Transport\n",
    "\n",
    "This tutorial implements some optimal transport based unsupervised domain adaptation methods, introduced by [[Courty *et al*, 2016](https://arxiv.org/pdf/1507.00504.pdf)]. In doing so, it also illustrates how to use the IterativeLinearSolver of OTT-JAX to solve linear transport problem with an extra (non-entropic) regularization term.  \n",
    "\n",
    "**The Domain Adaptation Setting**:\n",
    "1. We are given labeled *source data*, denoted $(x_s, y_s)$, and unlabeled *target data* $(x_t)$, where $x_s$ and $x_t$ live in different domains $\\Omega_s$ and $\\Omega_t$ of the same metric space $\\Omega$. \n",
    "1. We assume that the domain drift is due to an unknown, possibly nonlinear transformation of the target space $T\\colon \\Omega_t \\to \\Omega_s$, so that\n",
    "$\\mu_s = T_\\sharp \\mu_t,$ where  $\\mu_s$ and $\\mu_t$ denoting the source and the target marginal distribution over $x$, and that $T$ preserves the label information. \n",
    "1. As we have label information for the source data, we want to train a model (in the case of this tutorial a classifier), which we will then try to leverage to classify samples from the target domain\n",
    "\n",
    "The idea is then to:\n",
    "\n",
    "\n",
    "1. Estimate $\\mu_s$ and $\\mu_t$ with the corresponding empirical distributions.\n",
    "1. Find a transport map $T$ from $\\mu_t$ to $\\mu_s$, defined as the solution of a potentially regularized optimal transport problem.\n",
    "1. Transform target samples by inverting the transport map.\n",
    "1. Use the classifier trained on the source data and apply it on the transformed target samples.  \n",
    "\n",
    "In practice, we solve Kantorovitch formulation of the optimal transport problem, and add regularization, so that we get the following optimization problem:\n",
    "$$\\min_{\\gamma \\in \\mathcal B} \\langle \\gamma, C \\rangle_F +\\epsilon H(\\gamma) +\\eta \\Omega_c(\\gamma),$$\n",
    "where $\\mathcal B = \\{ \\gamma \\in (\\mathbb R_+)^{n_t \\times n_s}\\mid \\gamma 1_{n_s} = \\mu_t,\\, \\gamma^\\top 1_{n_t}=\\mu_s\\}$ is the set of probabilistic couplings between our empirical distributions, $H$ is the entropy,  and $\\Omega_c$ is a class-based regularizing term, that we define later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ott.solvers.generalized_contidional_gradient import GCG, GCGState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import random\n",
    "from jaxopt import BacktrackingLineSearch\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from ott.geometry import costs, pointcloud\n",
    "from ott.geometry.geometry import Geometry\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.problems.linear.linear_problem import LinearProblem\n",
    "from ott.solvers.linear import sinkhorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moon_data(num_s=300, num_t=300, angle=30):\n",
    "    \"\"\"\n",
    "    Generates the the source two moons dataset, then rotate it by\n",
    "    angle to generate the target\n",
    "    \"\"\"\n",
    "    x_s, y_s = datasets.make_moons(n_samples=num_s, noise=0.05, random_state=42)\n",
    "    theta = np.radians(angle)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array(((c, -s), (s, c)))\n",
    "    x_t = np.dot(x_s, R)\n",
    "    return jnp.array(x_s), jnp.array(y_s), jnp.array(x_t[:num_t])\n",
    "\n",
    "\n",
    "def get_gaussian_data(key, n_source=75, n_targets=75):\n",
    "    \"\"\"\n",
    "    Generates the source data of two 2D gaussian,\n",
    "    applies a linear transformation to create the target data\n",
    "    \"\"\"\n",
    "    key, *subkey = random.split(key, 5)\n",
    "    x_0 = jax.random.normal(subkey[0], (n_source, 2))\n",
    "    x_1 = 1.3 * jnp.ones((n_source, 2)) + jax.random.normal(\n",
    "        subkey[1], (n_targets, 2)\n",
    "    )\n",
    "    x_s = jnp.concatenate([x_0, x_1], axis=0)\n",
    "    y_s = jnp.concatenate(\n",
    "        [jnp.zeros(shape=(n_source,)), jnp.ones(shape=(n_targets,))], axis=0\n",
    "    )\n",
    "    x_t0 = 3 * jnp.ones((n_source, 2)) + jax.random.normal(\n",
    "        subkey[2], (n_targets, 2)\n",
    "    )\n",
    "    x_t1 = 7.6 * jnp.ones((n_source, 2)) + jax.random.normal(\n",
    "        subkey[3], (n_targets, 2)\n",
    "    )\n",
    "    x_t = jnp.concatenate([x_t0, x_t1], axis=0)\n",
    "    return jnp.array(x_s), jnp.array(y_s).astype(int), jnp.array(x_t)\n",
    "\n",
    "\n",
    "def train_plot(x_s, y_s, x_t, G=None):\n",
    "    \"\"\"\n",
    "    Train a Gaussian process classifier on the source data x_s, y_s.\n",
    "    If G=None, the classifier is then applied on the target data x_t\n",
    "    If G is not None, the classifier is also tested on the (scale normalized) transformation of x_t by G\n",
    "    The accurcay and classification zones are draw.\n",
    "    \"\"\"\n",
    "    if G == None:\n",
    "        n_samples = y_s.shape[0]\n",
    "        num_classes = np.bincount(y_s).shape[0]\n",
    "        plt.figure(1, figsize=(10, 10))\n",
    "        cm = plt.cm.RdBu\n",
    "        cm_bright = ListedColormap(\n",
    "            [\"#FF0000\", \"#0000FF\", \"#00FF00\"][:num_classes]\n",
    "        )\n",
    "        x_min = min(np.min(x_s[:, 0]), np.min(x_t[:, 0])) * 1.25\n",
    "        x_max = max(np.max(x_s[:, 0]), np.max(x_t[:, 0])) * 1.25\n",
    "        y_min = min(np.min(x_s[:, 1]), np.min(x_t[:, 1])) * 1.25\n",
    "        y_max = max(np.max(x_s[:, 1]), np.max(x_t[:, 1])) * 1.25\n",
    "        x_min, y_min = min(x_min, y_min), min(x_min, y_min)\n",
    "        x_max, y_max = max(x_max, y_max), max(x_max, y_max)\n",
    "\n",
    "        titles = [\"Source Data\", \"Target Data\"]\n",
    "        for i, x in enumerate([x_s, x_t]):\n",
    "            pl.subplot(2, 2, i + 1)\n",
    "            plt.scatter(\n",
    "                x[:, 0],\n",
    "                x[:, 1],\n",
    "                c=y_s,\n",
    "                cmap=cm_bright,\n",
    "                marker=\"p\",\n",
    "                edgecolor=\"k\",\n",
    "                label=\"_m\",\n",
    "            )\n",
    "            plt.xlim([x_min, x_max])\n",
    "            plt.ylim([y_min, y_max])\n",
    "            pl.title(titles[i])\n",
    "\n",
    "        # We train the classifier on source data\n",
    "        clf = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(x_s, y_s)\n",
    "\n",
    "        for i, x in enumerate([x_s, x_t]):\n",
    "            ax = pl.subplot(2, 2, i + 3)\n",
    "            plt.xlim([x_min, x_max])\n",
    "            plt.ylim([y_min, y_max])\n",
    "            pl.title(\"Classifier Performance on: \" + titles[i])\n",
    "            score = clf.score(x, y_s)\n",
    "            DecisionBoundaryDisplay.from_estimator(\n",
    "                clf,\n",
    "                np.concatenate((x_s, x_t)),\n",
    "                cmap=cm,\n",
    "                alpha=0.8,\n",
    "                ax=ax,\n",
    "                eps=2.0,\n",
    "            )\n",
    "            plt.scatter(\n",
    "                x[:, 0],\n",
    "                x[:, 1],\n",
    "                c=y_s,\n",
    "                cmap=cm_bright,\n",
    "                edgecolors=\"k\",\n",
    "                label=\"_m\",\n",
    "            )\n",
    "            ax.set_title(\"Classifier on: \" + titles[i])\n",
    "            ax.text(\n",
    "                0.9 * x_max,\n",
    "                0.9 * y_max,\n",
    "                \"score: \" + (\"%.2f\" % score).lstrip(\"0\"),\n",
    "                size=15,\n",
    "                horizontalalignment=\"right\",\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        n_samples = y_s.shape[0]\n",
    "        x_t_hat = n_samples * jnp.dot(G, x_s)\n",
    "        num_classes = np.bincount(y_s).shape[0]\n",
    "        plt.figure(1, figsize=(15, 10))\n",
    "        cm = plt.cm.RdBu\n",
    "        cm_bright = ListedColormap(\n",
    "            [\"#FF0000\", \"#0000FF\", \"#00FF00\"][:num_classes]\n",
    "        )\n",
    "        x_min = min(np.min(x_s[:, 0]), np.min(x_t[:, 0])) * 1.25\n",
    "        x_max = max(np.max(x_s[:, 0]), np.max(x_t[:, 0])) * 1.25\n",
    "        y_min = min(np.min(x_s[:, 1]), np.min(x_t[:, 1])) * 1.25\n",
    "        y_max = max(np.max(x_s[:, 1]), np.max(x_t[:, 1])) * 1.25\n",
    "        x_min, y_min = min(x_min, y_min), min(x_min, y_min)\n",
    "        x_max, y_max = max(x_max, y_max), max(x_max, y_max)\n",
    "\n",
    "        titles = [\"Source Data\", \"Target Data\", \"Transported Target Data\"]\n",
    "        for i, x in enumerate([x_s, x_t, x_t_hat]):\n",
    "            pl.subplot(2, 3, i + 1)\n",
    "            plt.scatter(\n",
    "                x[:, 0],\n",
    "                x[:, 1],\n",
    "                c=y_s,\n",
    "                cmap=cm_bright,\n",
    "                marker=\"p\",\n",
    "                edgecolor=\"k\",\n",
    "                label=\"_m\",\n",
    "            )\n",
    "            plt.xlim([x_min, x_max])\n",
    "            plt.ylim([y_min, y_max])\n",
    "            pl.title(titles[i])\n",
    "\n",
    "        # We train the classifier on source data\n",
    "        clf = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(x_s, y_s)\n",
    "\n",
    "        for i, x in enumerate([x_s, x_t, x_t_hat]):\n",
    "            ax = pl.subplot(2, 3, i + 4)\n",
    "            plt.xlim([x_min, x_max])\n",
    "            plt.ylim([y_min, y_max])\n",
    "            pl.title(\"Classifier Performance on: \" + titles[i])\n",
    "            score = clf.score(x, y_s)\n",
    "            DecisionBoundaryDisplay.from_estimator(\n",
    "                clf,\n",
    "                np.concatenate((x_s, x_t, x_t_hat)),\n",
    "                cmap=cm,\n",
    "                alpha=0.8,\n",
    "                ax=ax,\n",
    "                eps=2.0,\n",
    "            )\n",
    "            plt.scatter(\n",
    "                x[:, 0],\n",
    "                x[:, 1],\n",
    "                c=y_s,\n",
    "                cmap=cm_bright,\n",
    "                edgecolors=\"k\",\n",
    "                label=\"_m\",\n",
    "            )\n",
    "            ax.set_title(\"Classifier on: \" + titles[i])\n",
    "            ax.text(\n",
    "                0.9 * x_max,\n",
    "                0.9 * y_max,\n",
    "                \"score: \" + (\"%.2f\" % score).lstrip(\"0\"),\n",
    "                size=15,\n",
    "                horizontalalignment=\"right\",\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Approach "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider two toy source datasets: (1) the two semi-moon dataset (2) a 2D Gaussian mixture of 2 components. A transformation is applied to each to create the two target dataset. To illustrate the need for domain adapatation, a Gaussian process classifier is trained on the source data and is then used to classify the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moon Data\n",
    "mx_s, my_s, mx_t = get_moon_data()\n",
    "# Gaussian Data\n",
    "gx_s, gy_s, gx_t = get_gaussian_data(random.PRNGKey(42))\n",
    "train_plot(mx_s, my_s, mx_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot(gx_s, gy_s, gx_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adaptation with Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Conditional Gradient "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the transport map $T$ is done through solving an entropic regularized linear transport problem that is further regularized by $\\Omega_c$. We use the Generalized Conditional Gradient GCG [[Courty *et al*, 2016](https://arxiv.org/pdf/1507.00504.pdf)], which aims to minimize problems of the form \n",
    "$$g(\\gamma) = \\langle \\gamma, C \\rangle_F +\\epsilon H(\\gamma) +\\eta \\Omega_c(\\gamma)$$ \n",
    "\n",
    "by first setting $f(\\gamma)= \\langle \\gamma, C \\rangle_F  +\\eta \\Omega_c(\\gamma)$. Then iteratively \n",
    "\n",
    "1. Compute $\\mathbf{G}_k = \\nabla f(\\gamma_k)$\n",
    "1. find $\\gamma^*_k  = \\text{argmin}_{\\gamma \\in \\mathcal{B}} \\langle\\gamma,\\mathbf{G}_k \\rangle_F +\\varepsilon H(\\gamma)$\n",
    "1. Using a line search method, find $\\gamma_{k+1}$ by minimizing $g$ on the line segment between $\\gamma_k$ and $\\gamma^*_k$\n",
    "\n",
    "Looking at (2), GCG can be seen as a method iteratively solving a series of linear OT problems. OTT-JAX provides IterativeLinearSolver which allows the user to quickly implement methods that relies on iteratively solving linear problems. To use IterativeLinearSolver, the user needs to implement: \n",
    "1. ProblemState: a NamedTuple gathering problem specific state variables. This substate forms a part of the solver state. \n",
    "1. next_linear_pb: a function taking the current solver state (including ProblemState) and a random key to output a new linear problem.\n",
    "1. new_pstate_cost: a function that takes a solver state, a linear problem, solution to the linear problem, and a random key to output a new ProblemState and a cost associate with the current iteration.\n",
    "\n",
    "\n",
    "We now now implement a ProblemState and a general function to construct the ingredients  for GCG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider the Laplacian regularization. Given two positive and symmetric similarity matrices $S_s$ and $S_t$ for source data and target data, it is defined as\n",
    "$$\\Omega_c(\\gamma) = \\mathrm{Tr} (X_s^\\top \\gamma ^\\top L_t \\gamma X_s) + \\mathrm{Tr} (X_t^\\top \\gamma  L_s \\gamma^\\top X_t)$$\n",
    "where $L_s = \\mathrm{diag}(S_s1)-S_s$ (resp. $L_t$) is the Laplacian of the similarity matrix $S_s$ (resp. $S_t$). We compute $S_s$ and $S_t$ with scikit-learn function kneighbors_graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lap_reg(source_data, target_data, sim_param=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    ground_cost: the transport cost matrix of the underlying transport problem\n",
    "    source_data: the sample data of the source\n",
    "    target_data: the sample data of the target\n",
    "    sim_param:  parameter of the graph computations in the laplaciancs\n",
    "    alpha: relative weight on the two terms in laplacian regularization\n",
    "\n",
    "    return: the laplace loss function corresponding to the inputs\n",
    "    \"\"\"\n",
    "    # Initializing the Laplacian auxilary state\n",
    "    x_s, x_t = source_data, target_data\n",
    "\n",
    "    def laplacian(x):\n",
    "        return jnp.diag(jnp.sum(x, axis=0)) - jnp.array(x)\n",
    "\n",
    "    sS = kneighbors_graph(x_s, n_neighbors=int(sim_param)).toarray()\n",
    "    sT = kneighbors_graph(x_t, n_neighbors=int(sim_param)).toarray()\n",
    "    sS = jnp.array((sS + sS.T) / 2)\n",
    "    lS = laplacian(sS)\n",
    "    sT = jnp.array((sT + sT.T) / 2)\n",
    "    lT = laplacian(sT)\n",
    "\n",
    "    def laplace_loss(cost_mat):\n",
    "        a = alpha * jnp.trace(x_t.T @ cost_mat @ lS @ cost_mat.T @ x_t)\n",
    "        b = (1 - alpha) * jnp.trace(x_s.T @ cost_mat.T @ lT @ cost_mat @ x_s)\n",
    "        return a + b\n",
    "\n",
    "    return laplace_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 0.5\n",
    "epsilon = 10e-1\n",
    "\n",
    "# the Gaussian Mixture\n",
    "geom = pointcloud.PointCloud(gx_s, gx_t)\n",
    "cost_mat = geom.cost_matrix\n",
    "init_linear_pb = linear_problem.LinearProblem(Geometry(cost_mat, epsilon))\n",
    "lap_loss = lap_reg(gx_s, gx_t)\n",
    "\n",
    "solver = GCG(\n",
    "    epsilon=epsilon,\n",
    "    threshold=1e-3,\n",
    "    store_inner_errors=True,\n",
    ")\n",
    "\n",
    "last_state = solver(\n",
    "    cost_mat, lap_loss, epsilon, reg, init_linear_pb=init_linear_pb\n",
    ")\n",
    "\n",
    "last_solution = last_state.sol_matrix\n",
    "train_plot(gx_s, gy_s, gx_t, G=last_solution.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semi-Moon Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 0.5\n",
    "epsilon = 10e-1\n",
    "\n",
    "# the Gaussian Mixture\n",
    "geom = pointcloud.PointCloud(mx_s, mx_t)\n",
    "cost_mat = geom.cost_matrix\n",
    "init_linear_pb = linear_problem.LinearProblem(Geometry(cost_mat, epsilon))\n",
    "lap_loss = lap_reg(mx_s, mx_t)\n",
    "\n",
    "solver = GCG(\n",
    "    epsilon=epsilon,\n",
    "    threshold=1e-3,\n",
    "    store_inner_errors=True,\n",
    ")\n",
    "\n",
    "last_state = solver(\n",
    "    cost_mat, lap_loss, epsilon, reg, init_linear_pb=init_linear_pb\n",
    ")\n",
    "\n",
    "last_solution = last_state.sol_matrix\n",
    "\n",
    "train_plot(mx_s, my_s, mx_t, G=last_solution.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Sparsity Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider group sparsity regularization, defined as:\n",
    "$$ \\Omega_c (\\gamma) = \\sum_{i=1}^{n_{samples}} \\sum_{cl} \\|\\gamma(i,\\mathcal I_{cl})\\|_2,$$\n",
    "where $cl$ runs through all classes, and $I_{cl}$ contains the indices of columns in $\\gamma$ related to source domain samples of class $cl$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_reg(source_labels, class_num=2):\n",
    "    \"\"\"\n",
    "    source_labels: the labels of the datapoints in the source domain\n",
    "    class_num: the number of classes in the source domain\n",
    "    \"\"\"\n",
    "    # Initializing the Laplacian auxilary state\n",
    "    index_matrix = jnp.vstack([source_labels == i for i in range(class_num)])\n",
    "    index_matrix = jnp.expand_dims(index_matrix, 2)\n",
    "\n",
    "    def _sparse_reg(cost_mat):\n",
    "        cost_mat = jnp.expand_dims(cost_mat, axis=0)\n",
    "        res = jnp.sum(jnp.linalg.norm((cost_mat * index_matrix), axis=1))\n",
    "        iszero = jnp.isclose(res, 0)\n",
    "        res = jnp.where(iszero, 1, res)\n",
    "        res = res**0.5\n",
    "        res = jnp.where(iszero, 0, res)\n",
    "        return jnp.sum(res)\n",
    "\n",
    "    return _sparse_reg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the operations for sparse regularization are JITable, we can write a solver function and JIT it to improve the performance! This wasn't the case for Laplacian regularization because of the sklearn method but can be easily overcome by writing the function slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sparse_solver(x_s, y_s, x_t, reg=0.5, epsilon=10e-1):\n",
    "    geom = pointcloud.PointCloud(x_s, x_t)\n",
    "    cost_mat = geom.cost_matrix\n",
    "    init_linear_pb = linear_problem.LinearProblem(Geometry(cost_mat, epsilon))\n",
    "    sparse_loss = sparse_reg(y_s)\n",
    "    solver = GCG(\n",
    "        epsilon=epsilon,\n",
    "        threshold=1e-3,\n",
    "        store_inner_errors=True,\n",
    "    )\n",
    "\n",
    "    last_state = solver(\n",
    "        cost_mat, sparse_loss, epsilon, reg, init_linear_pb=init_linear_pb\n",
    "    )\n",
    "    return last_state.sol_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_mat = sparse_solver(gx_s, gy_s, gx_t)\n",
    "train_plot(gx_s, gy_s, gx_t, G=transform_mat.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semi-Moon Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_mat = sparse_solver(mx_s, my_s, mx_t)\n",
    "train_plot(mx_s, my_s, mx_t, G=transform_mat.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
